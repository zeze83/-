---
title: "Modeling"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
---

Here is for modeling

```{r setup,include=FALSE}
library(tidyverse)
library(readr)
library(caret)
library(MASS)
library(kableExtra)
```

```{r read data01,echo=FALSE}
diabetes_data01 <- read_csv("data/Diabetes_012_data.csv")

diabetes_data01 <- 
  diabetes_data01 |>
  dplyr::select(Diabetes_012, HighBP, HighChol, BMI, Smoker, 
                Income, Sex, PhysActivity, Fruits, Veggies) |>
  filter(Diabetes_012 == '2' | Diabetes_012 == '0') |>
  mutate(Diabetes_012 = ifelse(Diabetes_012 == "2", "1", Diabetes_012)) |>
  mutate(Diabetes_012 = as.numeric(Diabetes_012))
```

#### Logistic Regression Model

First, we split our data set into train and test data set in the portion of 2:1.

```{r splittraintest}
# Set seed for reproducibility
set.seed(1)

# Create indices for the train set
y <- diabetes_data01[["Diabetes_012"]] # Target column
trainIndex <- caret::createDataPartition(y, p = 0.67, list = FALSE)

# Create training and testing sets
train=diabetes_data01[trainIndex,]
test=diabetes_data01[-trainIndex,]
head(train)
head(test)
```

Then, we use train data set to fit our model with logistic regression.

```{r model validation}
# model fitting
model <- glm(Diabetes_012 ~ HighBP + HighChol + BMI + Smoker + Sex + Income + PhysActivity + Fruits + Veggies, data = train)
summary(model)

# Tidy the summary of the model
tidy_model <- broom::tidy(model)

# Use kable to create a clean table output
knitr::kable(tidy_model, format = "html", caption = "Logistic Regression Model Summary") |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))

# model validation
yhat <- predict(model, newdata = test, type = "response")
binary_predictions <- ifelse(yhat > 0.5, 1, 0)
head(binary_predictions)

# prediction accuracy
confusionMatrix(factor(binary_predictions), factor(test[["Diabetes_012"]]))
```

After that, we validate the model fitting in test data set. By calculating the accuracy, 85.89% shows that our logistic regression model performs pretty well.

```{r stepwise}
backward_model <- MASS::stepAIC(model, direction = "backward")
summary(backward_model)

# Tidy the summary of the model
tidy_backward <- broom::tidy(backward_model)

# Use kable to create a clean table output
knitr::kable(tidy_backward, format = "html", caption = "Backward Stepwise Model Summary") |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

Backward stepwise regression can sometimes exclude important variables or lead to overfitting. To double check the insignificant variable Smoker by using backward stepwise, which is the same as our logistic regression model, so we conclude that our model is the most appropriate model.
