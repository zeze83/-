---
title: "Modeling"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
---

```{r setup,include=FALSE}
library(tidyverse)
library(readr)
library(caret)
library(MASS)
library(kableExtra)
```

```{r read data01,echo=FALSE, message = FALSE}
diabetes_data01 <- read_csv("data/Diabetes_012_data.csv")

diabetes_data01 <- 
  diabetes_data01 |>
  dplyr::select(Diabetes_012, HighBP, HighChol, BMI, Smoker, 
                Income, Sex, PhysActivity, Fruits, Veggies) |>
  filter(Diabetes_012 == '2' | Diabetes_012 == '0') |>
  mutate(Diabetes_012 = ifelse(Diabetes_012 == "2", "1", Diabetes_012)) |>
  mutate(Diabetes_012 = as.numeric(Diabetes_012))
```

# Logistic Regression Model

First, we split our data set into train and test data set in the portion of 2:1.

```{r splittraintest}
# Set seed for reproducibility
set.seed(1)

# Create indices for the train set
y <- diabetes_data01[["Diabetes_012"]] # Target column
trainIndex <- caret::createDataPartition(y, p = 0.67, list = FALSE)

# Create training and testing sets
train=diabetes_data01[trainIndex,]
test=diabetes_data01[-trainIndex,]
head(train)
head(test)
```

# Model Fitting

Then, we use train data set to fit our model with logistic regression.

```{r model fitting}
# model fitting
model <- glm(Diabetes_012 ~ HighBP + HighChol + BMI + Smoker + Sex + Income + PhysActivity + Fruits + Veggies, data = train)
summary(model)

# Tidy the summary of the model
tidy_model <- broom::tidy(model)

# Use kable to create a clean table output
knitr::kable(tidy_model, format = "html", caption = "Logistic Regression Model Summary") |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

Backward stepwise regression can sometimes exclude important variables or lead to overfitting. To double check the insignificant variable Fruits by using backward stepwise, so we drop Fruits and conclude that this model is the most appropriate model.

```{r stepwise}
backward_model <- MASS::stepAIC(model, direction = "backward")
summary(backward_model)

# Tidy the summary of the model
tidy_backward <- broom::tidy(backward_model)

# Use kable to create a clean table output
knitr::kable(tidy_backward, format = "html", caption = "Backward Stepwise Model Summary") |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

# Model Validation

After that, we validate the model fitting in test data set. By calculating the accuracy, 85.89% shows that our logistic regression model performs pretty well.

```{r model validation}
# model validation
yhat <- predict(model, newdata = test, type = "response")
binary_predictions <- ifelse(yhat > 0.5, 1, 0)
print("Head data of y hat for binary prediction")
head(binary_predictions)
```

### Head data of y hat for binary prediction

```{r head data for y hat}
head(binary_predictions)
```

### Prediction Accuracy

```{r prediction accuracy}
# prediction accuracy
confusionMatrix(factor(binary_predictions), factor(test[["Diabetes_012"]]))
```

By calculating the accuracy, 85.89% shows that our logistic regression model performs pretty well. Therefore, we conclude that this model is the most appropriate model. 

Therefore, our final model is

Diabetes_binary = -0.050 + 0.123 HighBP +0.087 HighChol +0.008 BMI + 0.009 Smoker + 0.017 Sex - 0.017 Income - 0.036 PhysActivity - 0.008 Veggies.
